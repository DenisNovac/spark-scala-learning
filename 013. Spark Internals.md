# Spark internals

`RatingsCounter.scala`

Number of a given rating within movie set (100.000 movie ratings).

```scala
val sc = new SparkContext("local[*]", "RatingsCounter")

// 3
val lines: RDD[String] = sc.textFile("data/ml-100k/u.data")

// 2
val ratings: RDD[String] = lines.map(x => x.split("\t")(2))

// 1
val results: scala.collection.Map[String, Long] = ratings.countByValue()

val sortedResults: Seq[(String, Long)] = results.toSeq.sortBy(_._1)

sortedResults.foreach(println)
```

1 - when driver program got to the `countByValue` action - execution plan was created from RDD above. Needs to collect results in one place.

2,3 - reading and maping are applied in parallel across multiple computers or processes. 

So se have multiple stages:

  - Stage 1:
    - reading to file(3)
    - mapping(2)
  - Stage 2:
    - countByValue.

Each stage broken to parallel tasks distributed by executors. 
