# Spark SQL

Starting with Spark 2.
Modern API - Dataframes and Datasets. 
Layer on top of Spark Core (RDD).

Spark SQL allows to use data as an RDBMS table and optimize queries as SQL queries. So if using SQL with DataFrame - sometimes it'll give even more optimization.

## DataFrame

DataFrame is extension of RDD.
  
DataFrame looks more like actual DB:
  - more automatic optimizations than RDD;
  - contain Row objects;
  - can run SQL;
  - has schema (leading to efficient storage);
  - read and write to JSON, Hive, parquet...
  - commincates with JDBC, Tableau.

## DataSets

DataFrame is a DataSet of Row (`DataSet[Row]`).

DataSets can wrap a given type: `DataSet[(String, Double)]`.

**DataSet schema could be inferred at compile time (insted of runtime in DataFrame). Errors could be found before running.**

Even more optimization than DataFrame (some at compile time).

Can't be used in Python (no compile).

RDDs can be converted to DataSets with `.toDS()`.

SparkSession instead of SparkContext.

## Other features

Spark SQL exposes JDBC server (if used with Hive support);
port 10000 by default;
jdbc:hive2:localhost:10000

user-defined-function UDF:

```scala
val aquare = udf{ (x => x * x) }
```

