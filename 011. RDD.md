# Resilient Distributed Datasets

RDD - Core, original basic API from Spark 1.

Still effective for some solutions.

Not the same as just dataset in Spark. Datasets built on top of RDDs.

RDD - bunch of rows of data of some sort. Rows could be distributed on different computers. Resilient means the processing will be finished even if some node goes out, etc.

## Creation

First you need a Spark Context. One of the first things you would do in Driver program.

Examples of RDDS:

```scala
// rows: 1, 2, 3
// for big data use Range.inclusive(0, 999.999).toList
val nums = parallelize(List(1,2,3))

sc.textFile("file://...")
// or s3n:// , hdfs://

val hc = HiveContext(sc)
val rows = hc.sql("SELECT ...")

// JDBC, Cassandra, JSON, CSV, Elastic...
```

Basically any data loaded into context.

## Usage

What could be done with RDD:

- map (mapReduce, one row to one row);
- flatMap (one row to multiple rows);
- filter;
- distinct;
- sample;
- union, intersection, subtract, cartesian...

### Actions

RDD actions makes your RDD collapse and get result back to driver.

- collect;
- count;
- countByValue;
- take;
- top;
- reduce...

## Lazy Evaluation

In driver program nothing happens until you call an action on RDD (something like `RDD => PlainLocalType`). It has lazy evaluation strategy (Spark knows how to optimize different actions so it waits for it).


